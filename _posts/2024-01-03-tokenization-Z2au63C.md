---
layout: post
title: Tokenization
about: "Data tokenization is a substitution technique in which private or sensitive data elements are replaced with randomly generated alphanumeric strings. These strings or tokens have no value and canâ€™t be exploited. The original value or dataset cannot be reverse-engineered from a token value."
categories:
tags:
references:
  - https://en.wikipedia.org/wiki/Tokenization_(data_security)
  - https://cointelegraph.com/learn/what-is-data-tokenization
  - https://academy.binance.com/en/articles/what-is-data-tokenization-and-why-is-it-important
  - https://www.spiceworks.com/it-security/data-security/articles/what-is-tokenization
  - https://staxpayments.com/blog/data-tokenization
  - https://www.enov8.com/blog/data-tokenization-what-is-it
  - https://www.codezeros.com/what-is-data-tokenization-and-why-it-matters
  - https://www.linkedin.com/advice/0/what-key-features-data-tokenization-privacy
  - https://www.fortanix.com/blog/data-tokenization-best-practices-guide
  - https://www.forbes.com/sites/philippsandner/2021/07/06/data-tokenization-morphing-the-most-valuable-good-of-our-time-into-a-democratized-asset
  - https://datafloq.com/read/encryption-data-tokenization-securing-data
  - https://www.immuta.com/blog/what-is-data-tokenization
  - https://venafi.com/blog/what-is-tokenization
date: 2024-01-03T16:39:04.967Z
author: Giri Venkatesan
published: true
share: true
permalink: tokenization-Z2au63C
---
