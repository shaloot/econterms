---
layout: post
title: Shuffling
about: "Data shuffling is a process in modern data pipelines where the data is randomly redistributed across different partitions to enable parallel processing and better performance. Shuffling is generally done after some processing has been completed on the data, such as sorting or grouping, and before additional processing is performed. Shuffling can be an expensive operation in terms of time and resources, especially for large datasets."
categories:
tags:
references:
  - https://dagster.io/glossary/data-shuffling
  - https://www.talend.com/blog/data-privacy-shuffling-masking-part-1
date: 2024-01-03T16:39:04.954Z
author: Giri Venkatesan
published: true
share: true
permalink: shuffling-ZA5X4O
---
